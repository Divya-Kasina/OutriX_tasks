
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

url = "/content/Telco-Customer-Churn.csv"
data = pd.read_csv(url)

data.drop('customerID', axis=1, inplace=True)
data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')
data.dropna(inplace=True)

original_data = data.copy()  # Save original for visualizations

binary_cols = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Churn']
for col in binary_cols:
    data[col] = data[col].map({'Yes': 1, 'No': 0})
data.replace(['No internet service', 'No phone service'], 'No', inplace=True)
data = pd.get_dummies(data, drop_first=True)

X = data.drop('Churn', axis=1)
y = data['Churn']
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'SVM': SVC(probability=True),
    'KNN': KNeighborsClassifier()
}
colors = {
    'Logistic Regression': 'red',
    'Decision Tree': 'black',
    'Random Forest': 'green',
    'SVM': 'blue',
    'KNN': 'orange'
}

results = []
roc_data = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_prob) if y_prob is not None else None

    results.append({
        'Model': name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1,
        'AUC': auc
    })

    if y_prob is not None:
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        roc_data[name] = (fpr, tpr, colors[name])

results_df = pd.DataFrame(results)

print(" Model Performance:")
print(results_df.round(4))

# Plot metrics
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']
for metric in metrics:
    plt.figure(figsize=(8, 5))
    sns.barplot(x='Model', y=metric, data=results_df, palette='Set2', hue='Model', dodge=False)
    plt.title(f'{metric} Comparison')
    plt.ylabel(metric)
    plt.xticks(rotation=15)
    plt.tight_layout()
    plt.show()

# Plot ROC curves
plt.figure(figsize=(8, 6))
for name, (fpr, tpr, color) in roc_data.items():
    plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc_score(y_test, models[name].predict_proba(X_test)[:,1]):.2f})", color=color)
plt.plot([0, 1], [0, 1], 'k--')
plt.title("ROC Curve Comparison")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.grid()
plt.show()


# Count Plot: Churn Distribution
plt.figure(figsize=(6, 4))
sns.countplot(x='Churn', data=original_data) # Use original_data
plt.title("Churn Class Distribution")
plt.xticks([0, 1], ['No Churn', 'Churn'])
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# Count Plot: Churn vs Contract Type
plt.figure(figsize=(8, 4))
sns.countplot(data=original_data, x='Contract', hue='Churn') # Use original_data
plt.title("Churn by Contract Type")
plt.grid(axis='y')
plt.tight_layout()
plt.show()

# Correlation Heatmap
plt.figure(figsize=(10, 8))
corr = data.corr() # Use data for correlation after encoding
sns.heatmap(corr[['Churn']].sort_values(by='Churn', ascending=False), annot=True, cmap='coolwarm')
plt.title("Correlation of Features with Churn")
plt.tight_layout()
plt.show()

# Feature Importance (Random Forest)
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
importances = rf_model.feature_importances_
feat_names = X.columns
feat_imp_df = pd.DataFrame({'Feature': feat_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feat_imp_df.head(15), palette='magma')
plt.title("Top 15 Important Features (Random Forest)")
plt.tight_layout()
plt.show()

# Confusion Matrices for Each Model
from sklearn.metrics import ConfusionMatrixDisplay

for name, model in models.items():
    y_pred = model.predict(X_test)
    disp = ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, display_labels=['No Churn', 'Churn'], cmap='Purples')
    disp.ax_.set_title(f"{name} Confusion Matrix")
    plt.grid(False)
    plt.show()
